
page_title = 'Profile | Marcelo Peres'
page_icon = ':wave:'
name = 'Marcelo Peres'
role = 'Data Engineer & Analyst Professional'

info = '''\n
* - Git-lab-hub | CI/CD--Jenkins/Gitlab | Terraform
* - GCP--AWS | Storage | Athena/Redshift/Bigquery
* - ETL--ELT | Lambda/G functions/glue jobs/Datastage
* - Airflow | Sap | Oracle | API.
'''
email = 'brmarcelo.peres@gmail.com'
social_media = {
    'LinkedIn': 'https://www.linkedin.com/in/marcelo-peres-24611928/',
    'GitHub': 'https://github.com/Marcelo-Peres'
}

qualification = '## Experience & Qualifications'
qualification_info = '''
- âœ”ï¸ 8 Years of experience extracting actionable insights from data
- âœ”ï¸ Strong hands on experience and knowledge in Python, Excel and SQL
- âœ”ï¸ Good understanding of statistical principles and their respective applications
- âœ”ï¸ Excellent team-player and displaying strong sense of initiative on tasks
'''
skills = '## Hard Skills'
skills_info = '''
- ğŸ‘©â€ğŸ’» Programming: Python (Pandas), SQL, Pyspark & Spark
- ğŸ“Š Data Visulization: PowerBi, Tableau, Plotly
- ğŸ“š Modeling: Logistic regression, linear regression, decition trees
- ğŸ—„ï¸ Databases: Postgres, MySQL, SQL-Server & Oracle
'''
job_header = '## Work Experience'

job_t03 = '#### ğŸš§ Data Engineer | A3Data'
job_t03_time = '##### Dec 2022 - Mar 2023 | 04 months'
job_t03_info = '''<div style="text-align: justify;">
GCP - Google Cloud Platform\n
Stellantis - Datalake project

Responsible for creating tools to interect data at plataform.

- â–º Tools used:
    * âœ”ï¸ Terraform;
    * âœ”ï¸ Google Storage;
    * âœ”ï¸ BigQuery;
    * âœ”ï¸ Google Functions;
    * âœ”ï¸ Airflow;
    * âœ”ï¸ Python;
    * âœ”ï¸ SQL.
'''

job_t02 = '#### ğŸš§ Data Engineer Consultant | Via Consulting'
job_t02_time = '##### Apr 2022 - Sep 2022 | 06 months'
job_t02_info = '''<div style="text-align: justify;">\n
###### Via Consulting - Project - Zendesk Replication
* â–º Continuous pushing of data in a AWS environment using tools like:
    * âœ”ï¸ Pyspark Spark Glue jobs;
    * âœ”ï¸ Cloud Formation;
    * âœ”ï¸ AWS Athena;
    * âœ”ï¸ Apache Hudi metadata for data governance.

###### Unimed Insurance - Project - Stuffed Wallet (Carteira Recheada)

* â–º Colaborating with the team in a ETL process using tools like:
    * âœ”ï¸ informatica Powercenter;
    * âœ”ï¸ PLSQL - Oracle.
The idea of the project is a campaign that supports the score of the company's brokers.

###### Smiles S.A. - Project - Gol Spend & Get

* â–º AWS Python lambda function that validates a files to be called by an API.
    * << Resources >>:
    * âœ”ï¸ Cloud Formation;
    * âœ”ï¸ Python with unit tests;
    * âœ”ï¸ Github release info;
    * âœ”ï¸ Jenkins to observe github uploaded code;
    * âœ”ï¸ Sonar for code quality;
    * âœ”ï¸ End of Devops stack with deploy.
</div>'''

job_t01 = '#### ğŸš§ AWS & BI Consultant | BI4all'
job_t01_time = '##### Jul 2021 - May 2022 | 08 months'
job_t01_info = '''<div style="text-align: justify;">\n
###### Manserv Data Driven
* â–º AWS Management asset
    * âœ”ï¸ Responsible for creating data pipelines to push data in S3 using python AWS lambda funtions.
    * âœ”ï¸ Using packages such as awswrangler, xmltodict, json and much more.
    * âœ”ï¸ Being involved in great projects with the relevant skills, accessing different APIs from several providers.
    * âœ”ï¸ Some of these ones are Volvo, Nuntec, Komatsu - Komtrax, Caterpillar and many others.
    * âœ”ï¸ Transforming XML and JSON API extrations into tabular data to be recorded in parquet table format, grating a better use of S3 bucket as much as gaing performance in a compressed file format.
</div>'''
